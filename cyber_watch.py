#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import requests
import feedparser
from datetime import datetime, timedelta
import re
import json
from bs4 import BeautifulSoup
import time
import sys
from urllib.parse import urljoin, urlparse
import os
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed

class CyberWatchV3:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
        # Sources RSS par zone g√©ographique
        self.rss_sources = {
            'FRANCE': {
                'CERT-FR': 'https://www.cert.ssi.gouv.fr/feed/',
                'ANSSI': 'https://www.ssi.gouv.fr/feed/',
                'CNIL': 'https://www.cnil.fr/fr/rss.xml',
                'Zataz': 'https://www.zataz.com/feed/',
                'Undernews': 'https://www.undernews.fr/feed/',
                'IT-Connect': 'https://www.it-connect.fr/feed/',
                'Le Monde Informatique': 'https://www.lemondeinformatique.fr/flux-rss/securite.xml',
                'Journal du Net S√©curit√©': 'https://www.journaldunet.com/rss/securite.xml',
            },
            'INTERNATIONAL': {
                'Krebs on Security': 'https://krebsonsecurity.com/feed/',
                'BleepingComputer': 'https://www.bleepingcomputer.com/feed/',
                'SecurityWeek': 'https://www.securityweek.com/feed',
                'The Hacker News': 'https://thehackernews.com/feeds/posts/default',
                'Dark Reading': 'https://www.darkreading.com/rss/all.xml',
                'Threatpost': 'https://threatpost.com/feed/',
                'Security Affairs': 'https://securityaffairs.co/wordpress/feed',
                'CISA': 'https://www.cisa.gov/news.xml',
                'SANS ISC': 'https://isc.sans.edu/rssfeed.xml',
                'InfoSecurity Magazine': 'https://www.infosecurity-magazine.com/rss/news/',
                'SC Magazine': 'https://www.scmagazine.com/home/feed/',
                'Ars Technica Security': 'https://feeds.arstechnica.com/arstechnica/security',
                'Naked Security': 'https://nakedsecurity.sophos.com/feed/',
                'Malwarebytes Labs': 'https://blog.malwarebytes.com/feed/',
                'Kaspersky SecureList': 'https://securelist.com/feed/',
                'Mandiant': 'https://www.mandiant.com/resources/blog/rss.xml',
                'CrowdStrike': 'https://www.crowdstrike.com/blog/feed/',
                'Microsoft Security': 'https://www.microsoft.com/security/blog/feed/',
                'Google Security': 'https://security.googleblog.com/feeds/posts/default',
                'Cisco Talos': 'https://blog.talosintelligence.com/feeds/posts/default',
            }
        }
        
        # Sources web √† scraper
        self.web_sources = {
            'CERT-FR': 'https://www.cert.ssi.gouv.fr/',
            'US-CERT': 'https://us-cert.cisa.gov/ncas/alerts',
            'CVE Details': 'https://www.cvedetails.com/vulnerability-feeds.php',
        }
        
        # APIs publiques
        self.apis = {
            'nvd_cve': 'https://services.nvd.nist.gov/rest/json/cves/2.0',
            'mitre_cve': 'https://cve.mitre.org/data/downloads/allitems-cvrf.xml'
        }
        
        # Cat√©gories et mots-cl√©s
        self.categories = {
            'VULNERABILITIES': [
                'vulnerability', 'cve', 'exploit', 'patch', 'zero-day', 'rce', 'sql injection',
                'xss', 'buffer overflow', 'privilege escalation', 'remote code execution',
                'security flaw', 'security hole', 'backdoor', 'bypass'
            ],
            'MALWARE & THREATS': [
                'malware', 'ransomware', 'trojan', 'virus', 'worm', 'spyware', 'adware',
                'rootkit', 'botnet', 'rat', 'stealer', 'loader', 'cryptojacker',
                'banking trojan', 'apt', 'threat actor'
            ],
            'BREACHES & INCIDENTS': [
                'data breach', 'hack', 'attack', 'incident', 'compromised', 'stolen data',
                'leaked', 'cyberattack', 'security incident', 'breach', 'hacked',
                'unauthorized access', 'data leak', 'security breach'
            ],
            'TOOLS & TECHNIQUES': [
                'tool', 'technique', 'framework', 'methodology', 'pentest', 'red team',
                'blue team', 'forensics', 'incident response', 'security tool',
                'analysis', 'research', 'proof of concept', 'poc'
            ],
            'CRITICAL ALERTS': [
                'critical', 'urgent', 'emergency', 'immediate', 'high risk', 'severe',
                'widespread', 'active exploitation', 'in the wild', 'alert'
            ],
            'GENERAL NEWS': []  # Catch-all pour les autres articles
        }
        
        # Structure des articles par zone g√©ographique et cat√©gorie
        self.articles = {
            'FRANCE': {cat: [] for cat in self.categories.keys()},
            'INTERNATIONAL': {cat: [] for cat in self.categories.keys()}
        }
        
        # Date limite (24h)
        self.time_limit = datetime.now() - timedelta(hours=24)

    def print_banner(self):
        """Affiche une belle banni√®re"""
        banner = """
   _____      _                             _       _            ____  
  / ____|    | |                           | |     | |          |___ \ 
 | |    _   _| |__   ___ _ ____      ____ _| |_ ___| |__   __   ____) |
 | |   | | | | '_ \ / _ \ '__\ \ /\ / / _` | __/ __| '_ \  \ \ / /__ < 
 | |___| |_| | |_) |  __/ |   \ V  V / (_| | || (__| | | |  \ V /___) |
  \_____\__, |_.__/ \___|_|    \_/\_/ \__,_|\__\___|_| |_|   \_/|____/ 
         __/ |                                                         
        |___/                                                          
"""
        print(banner)

    def print_progress(self, current, total, source_name=""):
        """Affiche une barre de progression"""
        percent = (current / total) * 100
        bar_length = 40
        filled_length = int(bar_length * current // total)
        bar = '‚ñà' * filled_length + '‚ñë' * (bar_length - filled_length)
        
        print(f'\rüì° [{bar}] {percent:.1f}% - {source_name[:30]:<30}', end='', flush=True)

    def is_recent(self, date_str):
        """V√©rifie si un article est r√©cent (moins de 24h) avec formats de date √©tendus"""
        if not date_str:
            return True  # Si pas de date, on inclut l'article
            
        try:
            # Import dateutil pour un parsing plus robuste
            from dateutil import parser as date_parser
            
            # Nettoie la cha√Æne de date
            date_str = str(date_str).strip()
            
            # Formats courants de RSS/Atom
            formats = [
                '%a, %d %b %Y %H:%M:%S %z',       # RFC 2822
                '%a, %d %b %Y %H:%M:%S %Z',       # RFC 2822 avec timezone
                '%Y-%m-%dT%H:%M:%S%z',            # ISO 8601 avec timezone
                '%Y-%m-%dT%H:%M:%SZ',             # ISO 8601 UTC
                '%Y-%m-%dT%H:%M:%S.%f%z',         # ISO 8601 avec microsecondes
                '%Y-%m-%dT%H:%M:%S.%fZ',          # ISO 8601 UTC avec microsecondes
                '%Y-%m-%d %H:%M:%S',              # Format simple
                '%Y-%m-%d',                       # Date seule
                '%d %b %Y %H:%M:%S',              # Format alternatif
                '%d/%m/%Y %H:%M:%S',              # Format fran√ßais
                '%m/%d/%Y %H:%M:%S',              # Format am√©ricain
            ]
            
            article_date = None
            
            # Essaie d'abord avec dateutil (plus flexible)
            try:
                article_date = date_parser.parse(date_str)
            except:
                # Si dateutil √©choue, essaie les formats manuels
                for fmt in formats:
                    try:
                        article_date = datetime.strptime(date_str, fmt)
                        break
                    except:
                        continue
            
            if article_date:
                # Normalise la timezone
                if article_date.tzinfo is not None:
                    article_date = article_date.replace(tzinfo=None)
                
                return article_date >= self.time_limit
            
            return True  # Si parsing impossible, on inclut l'article
            
        except ImportError:
            # Fallback si dateutil n'est pas install√©
            try:
                for fmt in ['%a, %d %b %Y %H:%M:%S %z', '%Y-%m-%dT%H:%M:%SZ', '%Y-%m-%d %H:%M:%S']:
                    try:
                        article_date = datetime.strptime(str(date_str), fmt)
                        if article_date.tzinfo is not None:
                            article_date = article_date.replace(tzinfo=None)
                        return article_date >= self.time_limit
                    except:
                        continue
                return True
            except:
                return True

    def categorize_article(self, title, description=""):
        """Cat√©gorise un article selon son titre et sa description"""
        content = (title + " " + description).lower()
        
        for category, keywords in self.categories.items():
            if category == 'GENERAL NEWS':
                continue
            
            for keyword in keywords:
                if keyword.lower() in content:
                    return category
        
        return 'GENERAL NEWS'

    def clean_title(self, title):
        """Nettoie le titre des articles"""
        # Supprime les tags HTML
        title = re.sub(r'<[^>]+>', '', title)
        # Supprime les caract√®res sp√©ciaux en exc√®s
        title = re.sub(r'\s+', ' ', title).strip()
        # Limite la longueur
        if len(title) > 100:
            title = title[:97] + "..."
        return title

    def clean_description(self, description):
        """Nettoie la description des articles - supprime tout HTML"""
        if not description:
            return ""
        
        # Supprime tous les tags HTML
        description = re.sub(r'<[^>]+>', '', description)
        # D√©code les entit√©s HTML
        description = re.sub(r'&nbsp;', ' ', description)
        description = re.sub(r'&amp;', '&', description)
        description = re.sub(r'&lt;', '<', description)
        description = re.sub(r'&gt;', '>', description)
        description = re.sub(r'&quot;', '"', description)
        description = re.sub(r'&#039;', "'", description)
        # Supprime les espaces multiples et nettoie
        description = re.sub(r'\s+', ' ', description).strip()
        
        return description

    def fetch_single_rss_feed(self, source_name, rss_url, zone):
        """R√©cup√®re un seul flux RSS pour une zone g√©ographique donn√©e"""
        articles = {cat: [] for cat in self.categories.keys()}
        
        try:
            feed = feedparser.parse(rss_url)
            
            for entry in feed.entries[:50]:  # Limite √† 50 articles par source pour plus de contenu
                # V√©rification de la date
                pub_date = getattr(entry, 'published', getattr(entry, 'updated', ''))
                
                if not self.is_recent(pub_date):
                    continue
                
                title = self.clean_title(entry.title)
                link = entry.link
                raw_description = getattr(entry, 'summary', getattr(entry, 'description', ''))
                description = self.clean_description(raw_description)
                
                # Cat√©gorisation
                category = self.categorize_article(title, description)
                
                article = {
                    'title': title,
                    'link': link,
                    'source': source_name,
                    'date': pub_date,
                    'description': description[:200] if description else ''
                }
                
                articles[category].append(article)
                
        except Exception as e:
            pass
            
        return source_name, articles, zone

    def fetch_rss_feeds(self):
        """R√©cup√®re les flux RSS en parall√®le pour de meilleures performances"""
        print("üîç Recherche d'articles...")
        
        # Calcul du nombre total de sources
        total_sources = sum(len(sources) for sources in self.rss_sources.values())
        completed = 0
        
        # Thread lock pour la synchronisation
        lock = threading.Lock()
        
        def update_progress(source_name):
            nonlocal completed
            with lock:
                completed += 1
                self.print_progress(completed, total_sources, source_name)
        
        # Utilise ThreadPoolExecutor pour le traitement parall√®le
        with ThreadPoolExecutor(max_workers=8) as executor:
            # Soumet toutes les t√¢ches pour chaque zone
            future_to_info = {}
            
            for zone, sources in self.rss_sources.items():
                for source_name, rss_url in sources.items():
                    future = executor.submit(self.fetch_single_rss_feed, source_name, rss_url, zone)
                    future_to_info[future] = (source_name, zone)
            
            # Traite les r√©sultats au fur et √† mesure
            for future in as_completed(future_to_info):
                source_name, zone = future_to_info[future]
                update_progress(source_name)
                
                try:
                    _, articles, returned_zone = future.result()
                    
                    # Fusionne les articles dans les cat√©gories par zone
                    with lock:
                        for category, article_list in articles.items():
                            self.articles[zone][category].extend(article_list)
                            
                except Exception as e:
                    pass
        
        print()  # Nouvelle ligne apr√®s la barre de progression
        
        # Affiche le nombre d'articles r√©cup√©r√©s
        total_articles = 0
        for zone_articles in self.articles.values():
            for category_articles in zone_articles.values():
                total_articles += len(category_articles)
        
        print(f"‚úÖ {total_articles} articles r√©cup√©r√©s depuis {total_sources} sources")

    def fetch_cve_data(self):
        """R√©cup√®re les CVE r√©centes"""
        print("üîç R√©cup√©ration des CVE r√©centes...")
        
        try:
            # Date d'aujourd'hui pour l'API NVD
            today = datetime.now().strftime('%Y-%m-%d')
            yesterday = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')
            
            url = f"{self.apis['nvd_cve']}?pubStartDate={yesterday}&pubEndDate={today}"
            
            response = self.session.get(url, timeout=10)
            if response.status_code == 200:
                data = response.json()
                
                for vuln in data.get('vulnerabilities', [])[:15]:  # Limite √† 15 CVE
                    cve = vuln.get('cve', {})
                    cve_id = cve.get('id', '')
                    descriptions = cve.get('descriptions', [])
                    description = descriptions[0].get('value', '') if descriptions else ''
                    
                    title = f"CVE {cve_id}: {description[:80]}..." if len(description) > 80 else f"CVE {cve_id}: {description}"
                    link = f"https://nvd.nist.gov/vuln/detail/{cve_id}"
                    
                    article = {
                        'title': title,
                        'link': link,
                        'source': 'NVD',
                        'date': datetime.now().isoformat(),
                        'description': description[:200]
                    }
                    
                    self.articles['INTERNATIONAL']['VULNERABILITIES'].append(article)
                    
        except Exception as e:
            print(f"‚ö†Ô∏è  Erreur CVE: {str(e)}")

    def scrape_cert_fr(self):
        """Scrape les alertes CERT-FR"""
        print("üá´üá∑ R√©cup√©ration des alertes CERT-FR...")
        
        try:
            url = "https://www.cert.ssi.gouv.fr/alerte/"
            response = self.session.get(url, timeout=10)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # Recherche des alertes r√©centes
                alerts = soup.find_all('div', class_='alert-item')[:10]
                
                for alert in alerts:
                    try:
                        title_elem = alert.find('h3') or alert.find('a')
                        if title_elem:
                            title = self.clean_title(title_elem.get_text())
                            link_elem = alert.find('a')
                            link = urljoin(url, link_elem['href']) if link_elem and 'href' in link_elem.attrs else url
                            
                            article = {
                                'title': f"CERT-FR: {title}",
                                'link': link,
                                'source': 'CERT-FR',
                                'date': datetime.now().isoformat(),
                                'description': 'Alerte de s√©curit√© CERT-FR'
                            }
                            
                            category = self.categorize_article(title)
                            self.articles['FRANCE'][category].append(article)
                            
                    except:
                        continue
                        
        except Exception as e:
            print(f"‚ö†Ô∏è  Erreur CERT-FR: {str(e)}")

    def remove_duplicates(self):
        """Supprime les doublons par zone et cat√©gorie"""
        for zone in self.articles:
            for category in self.articles[zone]:
                seen_titles = set()
                unique_articles = []
                
                for article in self.articles[zone][category]:
                    title_key = article['title'].lower().strip()
                    if title_key not in seen_titles:
                        seen_titles.add(title_key)
                        unique_articles.append(article)
                
                self.articles[zone][category] = unique_articles

    def generate_terminal_output(self):
        """G√©n√®re la sortie pour le terminal selon le format FRANCE/INTERNATIONAL"""
        print("\n" + "="*80)
        print("üìä R√âSULTATS DE LA VEILLE CYBERS√âCURIT√â")
        print("="*80)
        
        # Calcul du total
        total_articles = 0
        for zone_articles in self.articles.values():
            for category_articles in zone_articles.values():
                total_articles += len(category_articles)
        
        print(f"üìà Total: {total_articles} articles trouv√©s\n")
        
        # Affichage par zone g√©ographique
        for zone in ['FRANCE', 'INTERNATIONAL']:
            zone_total = sum(len(articles) for articles in self.articles[zone].values())
            if zone_total > 0:
                flag = "üá´üá∑" if zone == "FRANCE" else "üåç"
                display_name = "FRAN√áAIS" if zone == "FRANCE" else "ANGLAIS"
                print(f"\n{flag} === {display_name} === ({zone_total} articles)")
                print("="*60)
                
                for category, articles in self.articles[zone].items():
                    if articles:
                        print(f"\nüîπ {category}")
                        print("-" * 50)
                        
                        for article in articles[:10]:  # Limite √† 10 par cat√©gorie pour l'affichage
                            title = article['title'][:70] + "..." if len(article['title']) > 70 else article['title']
                            print(f"  ‚Ä¢ {title}")
                            print(f"    üîó {article['link']}")
                            print(f"    üì∞ Source: {article['source']}\n")

    def generate_markdown_file(self):
        """G√©n√®re le fichier markdown selon le format FRANCE/INTERNATIONAL"""
        # Cr√©er le dossier rapports s'il n'existe pas
        reports_dir = "rapports"
        if not os.path.exists(reports_dir):
            os.makedirs(reports_dir)
        
        # Format chronologique optimal : YYYY-MM-DD_HH-MM-SS pour tri automatique
        timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
        filename = f"{reports_dir}/cyber_watch_{timestamp}.md"
        
        with open(filename, 'w', encoding='utf-8') as f:
            # En-t√™te
            f.write(f"# Veille Cybers√©curit√© - {datetime.now().strftime('%d/%m/%Y %H:%M')}\n\n")
            f.write("*Rapport automatis√© des derni√®res 24 heures*\n\n")
            
            # Sommaire global
            f.write("## Sommaire\n\n")
            
            total_global = 0
            for zone in ['FRANCE', 'INTERNATIONAL']:
                zone_total = sum(len(articles) for articles in self.articles[zone].values())
                if zone_total > 0:
                    display_name = "FRAN√áAIS" if zone == "FRANCE" else "ANGLAIS"
                    f.write(f"### {display_name} ({zone_total} articles)\n")
                    for category, articles in self.articles[zone].items():
                        if articles:
                            count = len(articles)
                            f.write(f"- **{category}**: {count} articles\n")
                    total_global += zone_total
                    f.write("\n")
            
            f.write(f"**Total global**: {total_global} articles\n\n")
            f.write("---\n\n")
            
            # Articles par zone et cat√©gorie
            for zone in ['FRANCE', 'INTERNATIONAL']:
                zone_total = sum(len(articles) for articles in self.articles[zone].values())
                if zone_total > 0:
                    flag = "üá´üá∑" if zone == "FRANCE" else "üåç"
                    display_name = "FRAN√áAIS" if zone == "FRANCE" else "ANGLAIS"
                    f.write(f"# {flag} {display_name}\n\n")
                    
                    for category, articles in self.articles[zone].items():
                        if articles:
                            f.write(f"## {category}\n\n")
                            
                            for i, article in enumerate(articles, 1):
                                title = article['title']
                                link = article['link']
                                source = article['source']
                                
                                # Format plus propre avec num√©rotation
                                f.write(f"{i}. **[{title}]({link})**\n")
                                f.write(f"   - Source: *{source}*\n")
                                
                                if article['description']:
                                    # Nettoie davantage la description
                                    clean_desc = article['description'][:150].strip()
                                    if clean_desc:
                                        f.write(f"   - Description: {clean_desc}...\n")
                                
                                f.write("\n")
            
            # Footer
            f.write(f"\n*G√©n√©r√© le {datetime.now().strftime('%d/%m/%Y √† %H:%M:%S')} par Cyber Watch v3.0*\n")
        
        return filename

    def run(self):
        """Ex√©cute la veille compl√®te"""
        self.print_banner()
        
        print("üöÄ D√©marrage de la recherche...")
        print(f"‚è∞ P√©riode: derni√®res 24h (depuis {self.time_limit.strftime('%d/%m/%Y %H:%M')})")
        print()
        
        # Collecte des donn√©es
        self.fetch_rss_feeds()
        
        self.fetch_cve_data()
        self.scrape_cert_fr()
        
        # Nettoyage
        print("üßπ Suppression des doublons...")
        self.remove_duplicates()
        
        # G√©n√©ration des sorties
        self.generate_terminal_output()
        
        print("\nüíæ G√©n√©ration du fichier markdown...")
        markdown_file = self.generate_markdown_file()
        
        print(f"‚úÖ Fichier g√©n√©r√©: {markdown_file}")
        print(f"üìç Chemin complet: {os.path.abspath(markdown_file)}")
        
        print("\nüéâ Veille termin√©e avec succ√®s!")

if __name__ == "__main__":
    try:
        cyber_watch = CyberWatchV3()
        cyber_watch.run()
    except KeyboardInterrupt:
        print("\n\n‚ùå Arr√™t demand√© par l'utilisateur")
        sys.exit(0)
    except Exception as e:
        print(f"\n\nüí• Erreur fatale: {str(e)}")
        sys.exit(1)